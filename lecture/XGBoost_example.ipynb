{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple XGBoost Example\n",
    "\n",
    "In this notebook, we show a very simple use pattern for XGBoost.  To run this, you need to 'pip install XGBoost' into your Python environment.\n",
    "\n",
    "author: Keith Chugg (chugg@usc.edu)\n",
    "\n",
    "ChatGPT was used in the generation of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "\n",
    "* Source: Wisconsin Diagnostic Breast Cancer (WDBC) dataset\n",
    "* Task: Binary classification (malignant vs. benign breast cancer)\n",
    "* Features: 30 numerical features computed from digitized images of fine needle aspirates of breast masses\n",
    "* Samples: 569 instances\n",
    "* Classes:\n",
    "- 0 = Malignant (cancerous)\n",
    "- 1 = Benign (non-cancerous)\n",
    "\n",
    "More details:  https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: shape: (455, 30)\n",
      "X_test: shape: (114, 30)\n",
      "\n",
      "y_train: shape: (455,)\n",
      "y_test: shape: (114,)\n",
      "\n",
      "Classses:  {0, 1}\n",
      "Class 1 (Benign) examples in train: 286  or  62.86% \n",
      "Class 1 (Benign) examples in test: 71 or  62.28% \n"
     ]
    }
   ],
   "source": [
    "print(f'X_train: shape: {X_train.shape}')\n",
    "print(f'X_test: shape: {X_test.shape}\\n')\n",
    "\n",
    "print(f'y_train: shape: {y_train.shape}')\n",
    "print(f'y_test: shape: {y_test.shape}\\n')\n",
    "\n",
    "print(f'Classses:  {set(y_train)}')\n",
    "print(f'Class 1 (Benign) examples in train: {np.sum(y_train)}  or {100 * np.mean(y_train) : 2.2f}% ')\n",
    "print(f'Class 1 (Benign) examples in test: {np.sum(y_test)} or {100 * np.mean(y_test) : 2.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train an XGBoost classifier\n",
    "model = xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Logistic Regression\n",
    "Let's run a quick comparison using logistic regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=10000)  # Increased iterations for convergence\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter OPtimization for XGBoost Using Optuna\n",
    "\n",
    "Let's use a package to optimize hyperparameters for XGBoost -- Optuna is one such package. Use `pip install optuna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 14:38:47,453] A new study created in memory with name: no-name-0e116ccd-d709-44b6-b115-ccf40e5fe940\n",
      "[I 2025-04-02 14:38:47,707] Trial 0 finished with value: 0.9494505494505494 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.051235878465405, 'subsample': 0.7132503251090343, 'colsample_bytree': 0.7960725516009777, 'gamma': 1.6580832525951839, 'lambda': 4.286563858249751, 'alpha': 5.601950464810335}. Best is trial 0 with value: 0.9494505494505494.\n",
      "[I 2025-04-02 14:38:48,242] Trial 1 finished with value: 0.9538461538461538 and parameters: {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.030410767263376645, 'subsample': 0.7827594339463035, 'colsample_bytree': 0.8088866042894138, 'gamma': 0.7003784483057618, 'lambda': 8.749398441356364, 'alpha': 7.295344857788613}. Best is trial 1 with value: 0.9538461538461538.\n",
      "[I 2025-04-02 14:38:48,493] Trial 2 finished with value: 0.9604395604395604 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.04089630023245055, 'subsample': 0.9532169988752098, 'colsample_bytree': 0.732086961815262, 'gamma': 1.8742067744240254, 'lambda': 0.2794178361479147, 'alpha': 0.7625084715496848}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:48,773] Trial 3 finished with value: 0.9406593406593406 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.011689084298470848, 'subsample': 0.720377150280134, 'colsample_bytree': 0.7456579928910617, 'gamma': 1.815818068885443, 'lambda': 6.105291297659389, 'alpha': 0.6418863000485237}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:49,140] Trial 4 finished with value: 0.945054945054945 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.18687900786330836, 'subsample': 0.9917592602669165, 'colsample_bytree': 0.8386727892405228, 'gamma': 3.9372356693539605, 'lambda': 8.032294801826769, 'alpha': 0.6459623103326878}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:49,352] Trial 5 finished with value: 0.9428571428571428 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.21289891869878264, 'subsample': 0.7220014405495577, 'colsample_bytree': 0.7846904138261149, 'gamma': 2.184321077607306, 'lambda': 2.2475847290338864, 'alpha': 9.842487833038096}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:49,574] Trial 6 finished with value: 0.9494505494505494 and parameters: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.16266041113499125, 'subsample': 0.983469237817162, 'colsample_bytree': 0.7706341114752053, 'gamma': 2.2509255231221568, 'lambda': 1.3882816274005816, 'alpha': 5.274165316171926}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:49,897] Trial 7 finished with value: 0.9406593406593406 and parameters: {'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.07578794616115886, 'subsample': 0.6262211959318305, 'colsample_bytree': 0.9527853094595009, 'gamma': 1.1420661244670993, 'lambda': 8.779195258526439, 'alpha': 9.229876948963009}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:50,112] Trial 8 finished with value: 0.9472527472527472 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.03910599987128996, 'subsample': 0.9196659790547355, 'colsample_bytree': 0.6327803510580122, 'gamma': 2.029926474597211, 'lambda': 8.51368208785087, 'alpha': 3.713269306746146}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:50,443] Trial 9 finished with value: 0.9516483516483516 and parameters: {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.09257015668893737, 'subsample': 0.7999348612044694, 'colsample_bytree': 0.8737498812811934, 'gamma': 0.34613685733084876, 'lambda': 3.4969572905006343, 'alpha': 4.579104482004493}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:50,592] Trial 10 finished with value: 0.9406593406593406 and parameters: {'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.017942531327596146, 'subsample': 0.9082391239433082, 'colsample_bytree': 0.6639575051418889, 'gamma': 3.6370863986213653, 'lambda': 0.5193696382317018, 'alpha': 2.4560589611173738}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:51,077] Trial 11 finished with value: 0.9538461538461538 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.028579616247692605, 'subsample': 0.8310438773318378, 'colsample_bytree': 0.700706062327851, 'gamma': 0.17742105547290254, 'lambda': 6.312503958564589, 'alpha': 7.76308146980677}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:51,509] Trial 12 finished with value: 0.9384615384615385 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.024144235934509005, 'subsample': 0.8214656769634864, 'colsample_bytree': 0.8982217100582134, 'gamma': 2.948901664315999, 'lambda': 9.891040703789175, 'alpha': 7.183652578531402}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:51,809] Trial 13 finished with value: 0.9494505494505494 and parameters: {'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.03899487827191065, 'subsample': 0.8876740600327443, 'colsample_bytree': 0.7184260437844527, 'gamma': 0.9564593307008956, 'lambda': 5.999489022764117, 'alpha': 7.405884102924651}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:51,934] Trial 14 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.08470574701427006, 'subsample': 0.7774367316176156, 'colsample_bytree': 0.9988255568624509, 'gamma': 2.9444921207466654, 'lambda': 2.8552023570958562, 'alpha': 2.2698051730081774}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:52,043] Trial 15 finished with value: 0.9472527472527472 and parameters: {'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.09877613059250559, 'subsample': 0.6065869179233901, 'colsample_bytree': 0.9969631318643569, 'gamma': 4.910181744963557, 'lambda': 2.6712508255705067, 'alpha': 2.1606563104748777}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:52,214] Trial 16 finished with value: 0.956043956043956 and parameters: {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.06537269283238051, 'subsample': 0.8687068594307998, 'colsample_bytree': 0.6048491813489139, 'gamma': 3.0101866913231277, 'lambda': 0.26108094375786184, 'alpha': 0.011596374332915715}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:52,378] Trial 17 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.13879958944972323, 'subsample': 0.7604821180680382, 'colsample_bytree': 0.9329921130318013, 'gamma': 2.9424386837603347, 'lambda': 1.7804636038639992, 'alpha': 2.141309619973506}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:52,520] Trial 18 finished with value: 0.9428571428571428 and parameters: {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.049710395334006036, 'subsample': 0.6745848599572941, 'colsample_bytree': 0.6860819008628077, 'gamma': 3.8388541065535184, 'lambda': 3.200044536822686, 'alpha': 3.398830942152114}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:52,738] Trial 19 finished with value: 0.9604395604395604 and parameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.2980131231560066, 'subsample': 0.9545659769051926, 'colsample_bytree': 0.8450372991543751, 'gamma': 1.4498880331918025, 'lambda': 1.0843188258124494, 'alpha': 1.5010892897176555}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:52,986] Trial 20 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.12052623897444438, 'subsample': 0.9450703049023493, 'colsample_bytree': 0.8427996082283845, 'gamma': 1.3632300683788312, 'lambda': 1.1182475287863736, 'alpha': 0.9574680157734123}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:53,143] Trial 21 finished with value: 0.9516483516483516 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.26655245306789876, 'subsample': 0.9606638326393647, 'colsample_bytree': 0.9953828271192593, 'gamma': 2.8387614395248586, 'lambda': 0.05398620065120585, 'alpha': 1.8434439376275822}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:53,341] Trial 22 finished with value: 0.9516483516483516 and parameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.29298228492752404, 'subsample': 0.8528485261778251, 'colsample_bytree': 0.7451968567858973, 'gamma': 2.508974833301483, 'lambda': 4.177499538092783, 'alpha': 3.18049174743291}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:53,665] Trial 23 finished with value: 0.9516483516483516 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.02090705596166339, 'subsample': 0.9504885758267647, 'colsample_bytree': 0.9080951486646526, 'gamma': 1.5163126432198346, 'lambda': 1.0638453200941784, 'alpha': 1.3707386293019557}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:53,838] Trial 24 finished with value: 0.9384615384615385 and parameters: {'n_estimators': 50, 'max_depth': 4, 'learning_rate': 0.013300328670721914, 'subsample': 0.9008393246341342, 'colsample_bytree': 0.9692924129686842, 'gamma': 3.5450942011055404, 'lambda': 2.143322235756762, 'alpha': 0.08917232438721356}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:54,137] Trial 25 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.06567063184554077, 'subsample': 0.7493328886561627, 'colsample_bytree': 0.8523580229713837, 'gamma': 2.5174160213064125, 'lambda': 2.7889333610947222, 'alpha': 2.882116041805113}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:54,340] Trial 26 finished with value: 0.9538461538461538 and parameters: {'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.1026702912855149, 'subsample': 0.8488814584020408, 'colsample_bytree': 0.7379629914707222, 'gamma': 4.27620862477343, 'lambda': 0.9860859133293989, 'alpha': 4.323434616200735}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:54,636] Trial 27 finished with value: 0.956043956043956 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.03936078949127399, 'subsample': 0.9256113912597075, 'colsample_bytree': 0.6637671433927381, 'gamma': 0.6542812358520604, 'lambda': 4.999465927855499, 'alpha': 1.346320037459702}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:54,743] Trial 28 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.23073475841018717, 'subsample': 0.9996949462597993, 'colsample_bytree': 0.9160738811477735, 'gamma': 1.8814172901190789, 'lambda': 1.808704395234222, 'alpha': 1.5654529443045408}. Best is trial 2 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:38:54,970] Trial 29 finished with value: 0.9494505494505494 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.053508204376604, 'subsample': 0.6763935787388378, 'colsample_bytree': 0.8156644869374278, 'gamma': 1.4984327148270942, 'lambda': 4.018235184799842, 'alpha': 5.778641035645987}. Best is trial 2 with value: 0.9604395604395604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.04089630023245055, 'subsample': 0.9532169988752098, 'colsample_bytree': 0.732086961815262, 'gamma': 1.8742067744240254, 'lambda': 0.2794178361479147, 'alpha': 0.7625084715496848}\n",
      "Optimized XGBoost Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10),\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params, eval_metric=\"logloss\") \n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    return scores.mean()\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)  # Run 30 trials\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Train final model using the best parameters\n",
    "best_model = xgb.XGBClassifier(**best_params, eval_metric=\"logloss\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized XGBoost Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
