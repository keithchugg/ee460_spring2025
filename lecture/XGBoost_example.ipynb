{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple XGBoost Example\n",
    "\n",
    "In this notebook, we show a very simple use pattern for XGBoost.  To run this, you need to 'pip install XGBoost' into your Python environment.\n",
    "\n",
    "author: Keith Chugg (chugg@usc.edu)\n",
    "\n",
    "ChatGPT was used in the generation of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset\n",
    "\n",
    "* Source: Wisconsin Diagnostic Breast Cancer (WDBC) dataset\n",
    "* Task: Binary classification (malignant vs. benign breast cancer)\n",
    "* Features: 30 numerical features computed from digitized images of fine needle aspirates of breast masses\n",
    "* Samples: 569 instances\n",
    "* Classes:\n",
    "- 0 = Malignant (cancerous)\n",
    "- 1 = Benign (non-cancerous)\n",
    "\n",
    "More details:  https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: shape: (455, 30)\n",
      "X_test: shape: (114, 30)\n",
      "\n",
      "y_train: shape: (455,)\n",
      "y_test: shape: (114,)\n",
      "\n",
      "Classses:  {0, 1}\n",
      "Class 1 (Benign) examples in train: 286  or  62.86% \n",
      "Class 1 (Benign) examples in test: 71 or  62.28% \n"
     ]
    }
   ],
   "source": [
    "print(f'X_train: shape: {X_train.shape}')\n",
    "print(f'X_test: shape: {X_test.shape}\\n')\n",
    "\n",
    "print(f'y_train: shape: {y_train.shape}')\n",
    "print(f'y_test: shape: {y_test.shape}\\n')\n",
    "\n",
    "print(f'Classses:  {set(y_train)}')\n",
    "print(f'Class 1 (Benign) examples in train: {np.sum(y_train)}  or {100 * np.mean(y_train) : 2.2f}% ')\n",
    "print(f'Class 1 (Benign) examples in test: {np.sum(y_test)} or {100 * np.mean(y_test) : 2.2f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train an XGBoost classifier\n",
    "model = xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Logistic Regression\n",
    "Let's run a quick comparison using logistic regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=10000)  # Increased iterations for convergence\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter OPtimization for XGBoost Using Optuna\n",
    "\n",
    "Let's use a package to optimize hyperparameters for XGBoost -- Optuna is one such package. Use `pip install optuna'\n",
    "\n",
    "You could also use grid search (e.g., `GridSearchCV` from sklearn), but Optuna uses a more sophisticated zero-order optimization technique and is generally more efficent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-02 14:53:08,645] A new study created in memory with name: no-name-9cff65c8-0f09-4ac3-8df2-d818f14e1491\n",
      "[I 2025-04-02 14:53:08,949] Trial 0 finished with value: 0.9494505494505494 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1646975871476703, 'subsample': 0.9732396819302107, 'colsample_bytree': 0.6071848655882724, 'gamma': 2.913471373403009, 'lambda': 0.9546463235629663, 'alpha': 4.624736085639256}. Best is trial 0 with value: 0.9494505494505494.\n",
      "[I 2025-04-02 14:53:09,095] Trial 1 finished with value: 0.9362637362637363 and parameters: {'n_estimators': 50, 'max_depth': 8, 'learning_rate': 0.023144240677034304, 'subsample': 0.6546200536072991, 'colsample_bytree': 0.8248673875255169, 'gamma': 0.6242570370149325, 'lambda': 8.166998359684966, 'alpha': 6.640090849873428}. Best is trial 0 with value: 0.9494505494505494.\n",
      "[I 2025-04-02 14:53:09,393] Trial 2 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.0809806639612142, 'subsample': 0.6515250360785294, 'colsample_bytree': 0.7973230422475382, 'gamma': 3.097519281070445, 'lambda': 6.278631858556758, 'alpha': 1.7635679718137143}. Best is trial 2 with value: 0.9582417582417582.\n",
      "[I 2025-04-02 14:53:09,793] Trial 3 finished with value: 0.9538461538461538 and parameters: {'n_estimators': 150, 'max_depth': 8, 'learning_rate': 0.014805487375345431, 'subsample': 0.6799680479864824, 'colsample_bytree': 0.8042171152994017, 'gamma': 0.21631765421719906, 'lambda': 3.6335648050774494, 'alpha': 2.9747921229797156}. Best is trial 2 with value: 0.9582417582417582.\n",
      "[I 2025-04-02 14:53:10,157] Trial 4 finished with value: 0.9604395604395604 and parameters: {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.2007251403988432, 'subsample': 0.7164961081283332, 'colsample_bytree': 0.943626132970787, 'gamma': 1.614987847988289, 'lambda': 6.884786319105905, 'alpha': 2.5941325667656328}. Best is trial 4 with value: 0.9604395604395604.\n",
      "[I 2025-04-02 14:53:10,741] Trial 5 finished with value: 0.9626373626373625 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.02567522953586102, 'subsample': 0.8361846098078649, 'colsample_bytree': 0.942764812016801, 'gamma': 0.44527831371646953, 'lambda': 2.811087864456184, 'alpha': 1.2256730698180829}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:11,209] Trial 6 finished with value: 0.9428571428571428 and parameters: {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.016605215944515764, 'subsample': 0.639184656724647, 'colsample_bytree': 0.7730216108962026, 'gamma': 4.353641553303706, 'lambda': 4.630146909026796, 'alpha': 7.155388876773847}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:11,351] Trial 7 finished with value: 0.9406593406593406 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.15052806772258587, 'subsample': 0.7896417807813634, 'colsample_bytree': 0.6851869948975907, 'gamma': 4.254449885119703, 'lambda': 1.6763146919731096, 'alpha': 6.188063022953634}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:11,612] Trial 8 finished with value: 0.956043956043956 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.05650292131932676, 'subsample': 0.7665219026006699, 'colsample_bytree': 0.7217702124198467, 'gamma': 1.6889603441363317, 'lambda': 2.580274053235528, 'alpha': 5.4594076788560555}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:12,040] Trial 9 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 150, 'max_depth': 5, 'learning_rate': 0.020831885952550887, 'subsample': 0.7032531418979083, 'colsample_bytree': 0.6349717455808418, 'gamma': 0.2896812164490137, 'lambda': 6.210603575928145, 'alpha': 0.258275575434864}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:12,366] Trial 10 finished with value: 0.9472527472527472 and parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.03277270380548473, 'subsample': 0.9092503632392405, 'colsample_bytree': 0.99851630132937, 'gamma': 1.682508701702177, 'lambda': 9.435351119500067, 'alpha': 9.99622670621807}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:12,636] Trial 11 finished with value: 0.9538461538461538 and parameters: {'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.2509478249598571, 'subsample': 0.8577861658921452, 'colsample_bytree': 0.9521022962865239, 'gamma': 1.4355043634771456, 'lambda': 6.3799670232582395, 'alpha': 2.3834504774864005}. Best is trial 5 with value: 0.9626373626373625.\n",
      "[I 2025-04-02 14:53:13,039] Trial 12 finished with value: 0.9692307692307691 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.040459536751322404, 'subsample': 0.7428904006870367, 'colsample_bytree': 0.9088127698005426, 'gamma': 0.9932675506954631, 'lambda': 3.8568616614781113, 'alpha': 0.06132368540752009}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:13,474] Trial 13 finished with value: 0.9626373626373628 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.03583284360862997, 'subsample': 0.8425748552420823, 'colsample_bytree': 0.8802199141934748, 'gamma': 0.8474372205693128, 'lambda': 0.15864410849419208, 'alpha': 0.27950530486036984}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:13,849] Trial 14 finished with value: 0.9604395604395604 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.04369344591752059, 'subsample': 0.7482083587525041, 'colsample_bytree': 0.8797266718140533, 'gamma': 1.09802640676212, 'lambda': 0.042148043898060816, 'alpha': 0.16369517733476172}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:14,197] Trial 15 finished with value: 0.9538461538461538 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.08776924068371321, 'subsample': 0.9030050254135534, 'colsample_bytree': 0.8859710404989658, 'gamma': 2.5578980560227773, 'lambda': 4.382255937282488, 'alpha': 3.765600761930971}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:14,520] Trial 16 finished with value: 0.9648351648351647 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.04688860595401826, 'subsample': 0.8280395965719474, 'colsample_bytree': 0.8678552645380742, 'gamma': 1.023175458716734, 'lambda': 0.27224409734440913, 'alpha': 0.9956791754596581}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:14,709] Trial 17 finished with value: 0.956043956043956 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.06533944303704299, 'subsample': 0.8088218892228112, 'colsample_bytree': 0.8469547034588438, 'gamma': 2.2473512550993195, 'lambda': 1.8774484614813725, 'alpha': 1.39424597041678}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:15,019] Trial 18 finished with value: 0.9384615384615385 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.010632514746194617, 'subsample': 0.6075318829218185, 'colsample_bytree': 0.9139462605119931, 'gamma': 3.6254663029891097, 'lambda': 3.243255873975925, 'alpha': 8.190069080539885}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:15,125] Trial 19 finished with value: 0.956043956043956 and parameters: {'n_estimators': 50, 'max_depth': 8, 'learning_rate': 0.10895575549933821, 'subsample': 0.7372673843363681, 'colsample_bytree': 0.9931049843686347, 'gamma': 2.1170830636873332, 'lambda': 5.120370336902404, 'alpha': 3.3704113801523308}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:15,397] Trial 20 finished with value: 0.956043956043956 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.04551599920133038, 'subsample': 0.8926596306462886, 'colsample_bytree': 0.7457201684295965, 'gamma': 1.0766774501394742, 'lambda': 1.350743557986723, 'alpha': 4.42074282624702}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:15,855] Trial 21 finished with value: 0.9626373626373628 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.03213761059266469, 'subsample': 0.8390976597349238, 'colsample_bytree': 0.870313495458358, 'gamma': 0.8098001097592684, 'lambda': 0.08843995548715879, 'alpha': 0.7066801956148792}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:16,383] Trial 22 finished with value: 0.9670329670329672 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.038412647434629694, 'subsample': 0.8008728687166103, 'colsample_bytree': 0.8962272804632365, 'gamma': 1.0322625434868113, 'lambda': 0.6212471981715524, 'alpha': 0.0021744807719081827}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:16,854] Trial 23 finished with value: 0.9692307692307691 and parameters: {'n_estimators': 150, 'max_depth': 10, 'learning_rate': 0.04613904147709342, 'subsample': 0.7843232354192743, 'colsample_bytree': 0.9145077550148809, 'gamma': 0.023050059667254263, 'lambda': 1.0950890728026286, 'alpha': 1.8657285123736336}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:17,214] Trial 24 finished with value: 0.9626373626373625 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.07017579053357396, 'subsample': 0.7838721010527283, 'colsample_bytree': 0.9155334662536706, 'gamma': 0.019016625761941824, 'lambda': 2.1108653722319954, 'alpha': 1.9256485452983552}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:17,451] Trial 25 finished with value: 0.956043956043956 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.10143082944879774, 'subsample': 0.75302725694654, 'colsample_bytree': 0.9613435422041733, 'gamma': 4.952333488460787, 'lambda': 3.9007986621581847, 'alpha': 0.03015443672393682}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:17,911] Trial 26 finished with value: 0.9648351648351647 and parameters: {'n_estimators': 250, 'max_depth': 10, 'learning_rate': 0.03508494942409716, 'subsample': 0.7147546061020724, 'colsample_bytree': 0.9156830132958902, 'gamma': 0.501020562560756, 'lambda': 1.0469635861212712, 'alpha': 1.9173768472298136}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:18,344] Trial 27 finished with value: 0.956043956043956 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.026928973688948355, 'subsample': 0.78895772247366, 'colsample_bytree': 0.8433631251976526, 'gamma': 0.00808304742882926, 'lambda': 2.5311417811494294, 'alpha': 0.966486860598288}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:18,661] Trial 28 finished with value: 0.9516483516483516 and parameters: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.05597059689939808, 'subsample': 0.9463178222290394, 'colsample_bytree': 0.9091904168501873, 'gamma': 1.315829638115933, 'lambda': 5.348662354367976, 'alpha': 3.846753790675401}. Best is trial 12 with value: 0.9692307692307691.\n",
      "[I 2025-04-02 14:53:19,027] Trial 29 finished with value: 0.9582417582417582 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1337411827889039, 'subsample': 0.873352504152704, 'colsample_bytree': 0.9754039700578782, 'gamma': 1.8722332657663585, 'lambda': 0.7548482595804695, 'alpha': 2.346747238537824}. Best is trial 12 with value: 0.9692307692307691.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.040459536751322404, 'subsample': 0.7428904006870367, 'colsample_bytree': 0.9088127698005426, 'gamma': 0.9932675506954631, 'lambda': 3.8568616614781113, 'alpha': 0.06132368540752009}\n",
      "\n",
      "Optimized XGBoost Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10),\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params, eval_metric=\"logloss\") \n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\") # use K-fold cross-validation for model selection with K=5\n",
    "    return scores.mean()\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)  # Run 30 trials\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print('\\n\\nBest parameters:', best_params)\n",
    "\n",
    "# Train final model using the best parameters\n",
    "best_model = xgb.XGBClassifier(**best_params, eval_metric=\"logloss\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nOptimized XGBoost Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
