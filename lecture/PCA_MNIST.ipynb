{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructor**: Prof. Keith Chugg (chugg@usc.edu)\n",
    "\n",
    "**Teaching Assistant**: Alexios Rustom (arustom@usc.edu)\n",
    "\n",
    "**Book**: Watt, J., Borhani, R., & Katsaggelos, A. K. (2020). Machine learning refined: Foundations, algorithms, and applications. Cambridge University Press.\n",
    "\n",
    "**Notebooks**: Written by Alexios Rustom (arustom@usc.edu) and Prof. Keith Chugg (chugg@usc.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MNIST_data(data_path, fashion=False, quiet=False):\n",
    "    if not fashion:\n",
    "        train_set = datasets.MNIST(data_path, download=True, train=True)\n",
    "        test_set = datasets.MNIST(data_path, download=True, train=False)\n",
    "    else:\n",
    "        train_set = datasets.FashionMNIST(data_path, download=True, train=True)\n",
    "        test_set = datasets.FashionMNIST(data_path, download=True, train=False)      \n",
    "    x_train = train_set.data.numpy()\n",
    "    y_train = train_set.targets.numpy()\n",
    "\n",
    "    x_test = test_set.data.numpy()\n",
    "    y_test = test_set.targets.numpy()\n",
    "    \n",
    "    N_train, H, W = x_train.shape\n",
    "    N_test, H, W = x_test.shape\n",
    "\n",
    "    if not quiet:\n",
    "        print(f'The data are {H} x {W} grayscale images.')\n",
    "        print(f'N_train = {N_train}')\n",
    "        print(f'N_test = {N_test}')\n",
    "    for i in set(y_train):\n",
    "        N_i_train = np.sum(y_train==i)\n",
    "        N_i_test = np.sum(y_test==i)\n",
    "        if not quiet:\n",
    "            print(f'Class {i}: has {N_i_train} train images ({100 * N_i_train / N_train : .2f} %), {N_i_test} test images ({100 * N_i_test/ N_test : .2f} %) ')\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_FASHION_MNIST = False\n",
    "if USE_FASHION_MNIST:\n",
    "    tag_name = 'FashionMNIST'\n",
    "else:\n",
    "    tag_name = 'MNIST'\n",
    "    \n",
    "x_train, y_train, x_test, y_test = load_MNIST_data('./data/', fashion=USE_FASHION_MNIST, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray() # B/W Images\n",
    "plt.figure(figsize = (10,9)) # Adjusting figure size\n",
    "# Displaying a grid of 3x3 images\n",
    "for i in range(9):\n",
    "    index = np.random.randint(low=0, high=len(y_train), dtype=int)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_train[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_float = x_train.astype(\"float32\") \n",
    "x_test_float = x_test.astype(\"float32\")\n",
    "# Normalization\n",
    "x_train_normalized = x_train_float/255.0\n",
    "x_test_normalized = x_test_float/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train_normalized.reshape(len(x_train_normalized),-1) #reshape to fed into PCA\n",
    "X_test = x_test_normalized.reshape(len(x_test_normalized),-1) #reshape to fed into PCA\n",
    "\n",
    "print(f'x_train_normalized.shape: {x_train_normalized.shape}')\n",
    "print(f'X_train.shape: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The linear Autoencoder cost may have many minimizers, of which the set of principal components is a particularly important one. \n",
    "\n",
    "\n",
    "- The spanning set of principal components always provide a consistent skeleton for a dataset, with its members pointing in the datasetâ€™s *largest directions of orthogonal variance*. \n",
    "\n",
    "\n",
    "- Employing this particular solution to the linear Autoencoder is often referred to as Principal Component Analysis, or PCA for short, in practice.\n",
    "\n",
    "- This idea is illustrated for a prototypical $N = 2$ dimensional dataset in Figure 8.6 (book), where the general elliptical distribution of the data is shown in light grey. \n",
    "\n",
    "\n",
    "- A scaled version of the first principal component of this dataset points in the direction in which the dataset is most spread out, also called its largest direction of variance. \n",
    "\n",
    "\n",
    "- A scaled version of the second principal component points in the next most important direction in which the dataset is spread out that is orthogonal to the first.\n",
    "\n",
    "- This special orthonormal minimizer of the linear Autoencoder is given by the eigenvectors of the so-called covariance matrix of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denoting by $\\mathbf{X}$ the $N \\times P$ data matrix consisting of our $P$ mean-centered input points stacked column-wise\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "\\vert  \\,\\,\\,\\,\\,\\,\\, \\vert  \\,\\,\\,\\,\\,  \\cdots  \\,\\,\\,\\,\\, \\vert \\\\\n",
    "\\,\\, \\mathbf{x}_1 \\,\\,\\, \\mathbf{x}_2 \\,\\,\\, \\cdots \\,\\,\\, \\mathbf{x}_P \\\\\n",
    "\\vert  \\,\\,\\,\\,\\,\\,\\, \\vert  \\,\\,\\,\\,\\,  \\cdots  \\,\\,\\,\\,\\, \\vert \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "- The orthogonal basis provided by this special solution (called the *Principal Components* of a dataset) can be computed (as a minimum of the Autoencoder cost function) as the *eigenvectors* of the corresponding *correlation matrix* of this data\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{covariance matrix of } \\, \\mathbf{X}: = \\, \\frac{1}{P}\\mathbf{X}^{\\,} \\mathbf{X}^T\n",
    "\\end{equation}\n",
    "\n",
    "- Denoting the eigenvector/value decomposition of the covariance matrix \n",
    "$\\frac{1}{P}\\mathbf{X}^{\\,} \\mathbf{X}^T$ is given as\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P}\\mathbf{X}^{\\,} \\mathbf{X}^T = \\mathbf{V}^{\\,}\\mathbf{D}^{\\,}\\mathbf{V}^T\n",
    "\\end{equation}\n",
    "\n",
    "- then above the orthonormal basis we recover is given precisely by the eigenvectors above, i.e., $\\mathbf{C} = \\mathbf{V}$.  \n",
    "\n",
    "\n",
    "- Again, these are referred to in the jargon of machine learning as the *principal components* of the data.\n",
    "\n",
    "- Moreover, the variance in each (principal component) direction is given precisely by the corresponding eigenvalue in $\\mathbf{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for reduced dimensional representation\n",
    "\n",
    "Now, let's apply PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = X_train.shape[0]\n",
    "Kx = X_train.T @ X_train / P ## sample covariance matrix\n",
    "\n",
    "e_vals, E_vecs = np.linalg.eig(Kx)\n",
    "\n",
    "E_vecs = E_vecs.T\n",
    "small_to_big = np.argsort(e_vals)\n",
    "big_to_small = small_to_big[::-1]\n",
    "\n",
    "e_vals = e_vals[big_to_small]\n",
    "E_vecs = E_vecs[big_to_small]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basis of orthonormal eigenvectors of ${\\bf X}^T {\\bf X}$, we can express a given data vector\n",
    "\n",
    "Let's see how the eigenvalues compare in value. \n",
    "\n",
    "\n",
    " Also, the approximation is an orthogonal project, so \n",
    "\\[\n",
    "    \\| {\\bf x} - \\hat{\\bf x} \\|^2 = \\| \\sum_{k=0}^{P-1} X_k |bf e_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lambdas: {e_vals[:10]}')\n",
    "plt.figure()\n",
    "plt.stem(10 * np.log10(e_vals[:47]))\n",
    "plt.grid(';')\n",
    "plt.xlabel('eigen value index (max to min)')\n",
    "plt.ylabel('eigen value in dB')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(10 * np.log10(e_vals))\n",
    "plt.grid(';')\n",
    "plt.xlabel('eigen value index (max to min)')\n",
    "plt.ylabel('eigen value in dB')\n",
    "\n",
    "H_W = len(e_vals)\n",
    "percent_energy = np.zeros(H_W)\n",
    "for k in range(H_W):\n",
    "    percent_energy[k] = np.sum(e_vals[:k])\n",
    "percent_energy = 100 * percent_energy / np.sum(e_vals)\n",
    "\n",
    "print(percent_energy.shape)\n",
    "plt.figure()\n",
    "plt.plot(np.arange(H_W), percent_energy, color='b', label='energy')\n",
    "plt.plot(np.arange(H_W), 100 - percent_energy, color='r', label='error energy')\n",
    "plt.axhline(95, color='b', linestyle='--', label='95%')\n",
    "plt.axhline(5, color='r', linestyle='--', label='5%')\n",
    "plt.grid(';')\n",
    "plt.legend()\n",
    "plt.xlim([0,200])\n",
    "plt.xlabel('eigen value index (max to min)')\n",
    "plt.ylabel('Percent Energy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = 50\n",
    "coeff =  (E_vecs[:k_max] @ X_train.T).T\n",
    "X_approx = coeff @ E_vecs[:k_max]\n",
    "X_approx = X_approx.reshape(P, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray() # B/W Images\n",
    "plt.figure(figsize = (10,9))\n",
    "# Displaying a grid of 3x3 images\n",
    "for i in range(9):\n",
    "    index = np.random.randint(low=0, high=len(y_train), dtype=int)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_approx[index])\n",
    "    plt.title(\"Number:{}\".format(y_train[index]),fontsize = 17)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mls23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
